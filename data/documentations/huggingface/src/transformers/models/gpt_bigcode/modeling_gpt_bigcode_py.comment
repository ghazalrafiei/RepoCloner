['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2023 The Bigcode team and HuggingFace Inc. team.','line_number':2,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':3,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':4,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':5,'multiline':False]
['text':'','line_number':6,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':7,'multiline':False]
['text':'','line_number':8,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':9,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':10,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':11,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':12,'multiline':False]
['text':' limitations under the License.','line_number':13,'multiline':False]
['text':' noqa','line_number':46,'multiline':False]
['text':' See all GPTBigCode models at https://huggingface.co/models?filter=gpt_bigcode','line_number':56,'multiline':False]
['text':' Fused kernels','line_number':60,'multiline':False]
['text':' Use separate functions for each case because conditionals prevent kernel fusion.','line_number':61,'multiline':False]
['text':' TODO: Could have better fused kernels depending on scaling, dropout and head mask.','line_number':62,'multiline':False]
['text':'  Is it doable without writing 32 functions?','line_number':63,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama._get_unpad_data','line_number':90,'multiline':False]
['text':' torch.where expects a tensor. We use a cache to avoid recreating it every time.','line_number':149,'multiline':False]
['text':' MQA models: (batch_size, query_length, num_heads * head_dim)','line_number':164,'multiline':False]
['text':' MHA models: (batch_size, num_heads, query_length, head_dim)','line_number':165,'multiline':False]
['text':' (batch_size, query_length, num_heads, head_dim) x (batch_size, head_dim, key_length)','line_number':170,'multiline':False]
['text':' -> (batch_size, query_length, num_heads, key_length)','line_number':171,'multiline':False]
['text':' No copy needed for MQA 2, or when layer_past is provided.','line_number':175,'multiline':False]
['text':' (batch_size, num_heads, query_length, head_dim) x (batch_size, num_heads, head_dim, key_length)','line_number':178,'multiline':False]
['text':' -> (batch_size, num_heads, query_length, key_length)','line_number':179,'multiline':False]
['text':' Always copies','line_number':183,'multiline':False]
['text':' No copy when layer_past is provided.','line_number':185,'multiline':False]
['text':' This is needed because of a bug in pytorch https://github.com/pytorch/pytorch/issues/80588.','line_number':190,'multiline':False]
['text':' The bug was fixed in https://github.com/pytorch/pytorch/pull/96086,','line_number':191,'multiline':False]
['text':' but the fix has not been released as of pytorch version 2.0.0.','line_number':192,'multiline':False]
['text':' Use a fused kernel to prevent a large overhead from casting and scaling.','line_number':200,'multiline':False]
['text':' Sub-optimal when the key length is not a multiple of 8.','line_number':201,'multiline':False]
['text':' The fused kernel is very slow when the key length is not a multiple of 8, so we skip fusion.','line_number':211,'multiline':False]
['text':' Mask heads if we want to','line_number':218,'multiline':False]
['text':' Note: We split as (self.num_heads, 3, self.head_dim) instead of (3, self.num_heads, self.head_dim),','line_number':258,'multiline':False]
['text':' i.e., the memory layout is not the same as GPT2.','line_number':259,'multiline':False]
['text':' This makes the concatenation with past_key_value more efficient.','line_number':260,'multiline':False]
['text':' Transpose to return weights in the usual format (batch_size, num_heads, query_length, key_length)','line_number':284,'multiline':False]
['text':' a, present, (attentions)','line_number':288,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__','line_number':298,'multiline':False]
['text':' TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.','line_number':302,'multiline':False]
['text':' flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.','line_number':303,'multiline':False]
['text':' Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).','line_number':304,'multiline':False]
['text':' Note: We split as (self.num_heads, 3, self.head_dim) instead of (3, self.num_heads, self.head_dim),','line_number':334,'multiline':False]
['text':' i.e., the memory layout is not the same as GPT2.','line_number':335,'multiline':False]
['text':' This makes the concatenation with past_key_value more efficient.','line_number':336,'multiline':False]
['text':' Flash attention requires the input to have the shape','line_number':350,'multiline':False]
['text':' batch_size x seq_length x head_dim x hidden_dim','line_number':351,'multiline':False]
['text':' In PEFT, usually we cast the layer norms in float32 for training stability reasons','line_number':373,'multiline':False]
['text':' therefore the input hidden states gets silently casted in float32. Hence, we need','line_number':374,'multiline':False]
['text':' cast them back in float16 just to be sure everything works as expected.','line_number':375,'multiline':False]
['text':' Handle the case where the model is quantized','line_number':378,'multiline':False]
['text':' Transpose to return weights in the usual format (batch_size, num_heads, query_length, key_length)','line_number':405,'multiline':False]
['text':' a, present, (attentions)','line_number':412,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2._flash_attention_forward','line_number':414,'multiline':False]
['text':' TODO: Remove the `query_length != 1` check once Flash Attention for RoCm is bumped to 2.1. For details, please see the comment in LlamaFlashAttention2 __init__.','line_number':440,'multiline':False]
['text':' Contains at least one padding token in the sequence','line_number':443,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2._upad_input','line_number':474,'multiline':False]
['text':' There is a memcpy here, that is very bad.','line_number':496,'multiline':False]
['text':' The -q_len: slice assumes left padding.','line_number':500,'multiline':False]
['text':' The super dispatch is done in the forward.','line_number':517,'multiline':False]
['text':' MQA models: (batch_size, query_length, num_heads * head_dim)','line_number':526,'multiline':False]
['text':' MHA models: (batch_size, num_heads, query_length, head_dim)','line_number':527,'multiline':False]
['text':' SDPA requires the dimension [..., sequence_length, head_dim].','line_number':535,'multiline':False]
['text':' Without these unsqueeze, SDPA complains as the query and key/value have a different number of dimensions.','line_number':538,'multiline':False]
['text':' Although these expand are not numerically useful, PyTorch 2.1 can not dispatch to memory-efficient backend','line_number':542,'multiline':False]
['text':' and flash attention backend (No available kernel.  Aborting execution.) from the shapes','line_number':543,'multiline':False]
['text':' query = [batch_size, num_heads, query_length, head_dim]','line_number':544,'multiline':False]
['text':' key = [batch_size, 1, past_length, head_dim]','line_number':545,'multiline':False]
['text':' value = [batch_size, 1, past_length, head_dim]','line_number':546,'multiline':False]
['text':'','line_number':547,'multiline':False]
['text':' so we could do:','line_number':548,'multiline':False]
['text':'','line_number':549,'multiline':False]
['text':' key = key.expand(-1, self.num_heads, -1, -1)','line_number':550,'multiline':False]
['text':' value = value.expand(-1, self.num_heads, -1, -1)','line_number':551,'multiline':False]
['text':'','line_number':552,'multiline':False]
['text':' However SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,','line_number':553,'multiline':False]
['text':' so we always dispatch to the math path: https://github.com/pytorch/pytorch/issues/112577.','line_number':554,'multiline':False]
['text':' Arguably we could still do expand + contiguous when `query.device.type == "cuda"` in order to dispatch on memory-efficient','line_number':555,'multiline':False]
['text':' backend, but it feels very hacky.','line_number':556,'multiline':False]
['text':' See the comment above.','line_number':560,'multiline':False]
['text':' The query_length > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case query_length == 1.','line_number':572,'multiline':False]
['text':' (batch_size, num_heads, seq_len, head_dim) --> (batch_size, seq_len, num_heads, head_dim)','line_number':578,'multiline':False]
['text':' Reshape is kind of expensive here, as it does a memory copy,','line_number':581,'multiline':False]
['text':' but I did not manage to make away without it (logits do not match when using view)','line_number':582,'multiline':False]
['text':' (batch_size, seq_len, num_heads, head_dim) --> (batch_size, seq_len, num_heads * head_dim)','line_number':583,'multiline':False]
['text':' Note: We split as (self.num_heads, 3, self.head_dim) instead of (3, self.num_heads, self.head_dim),','line_number':615,'multiline':False]
['text':' i.e., the memory layout is not the same as GPT2.','line_number':616,'multiline':False]
['text':' This makes the concatenation with past_key_value more efficient.','line_number':617,'multiline':False]
['text':' Difference with the original implementation: there is no need to transpose the key here,','line_number':632,'multiline':False]
['text':' as SDPA expects seq_length to be at index -2 for the key as well','line_number':633,'multiline':False]
['text':' TODO: Improve this warning with e.g. `model.config._attn_implementation = "manual"` once this is implemented.','line_number':636,'multiline':False]
['text':' Transpose to return weights in the usual format (batch_size, num_heads, query_length, key_length)','line_number':651,'multiline':False]
['text':' Copied from transformers.models.gpt2.modeling_gpt2.GPT2MLP.forward','line_number':667,'multiline':False]
['text':' output_attn: a, present, (attentions)','line_number':730,'multiline':False]
['text':' residual connection','line_number':732,'multiline':False]
['text':' add one self-attention block for cross-attention','line_number':736,'multiline':False]
['text':' residual connection','line_number':753,'multiline':False]
['text':' add cross attentions if we output attention weights','line_number':755,'multiline':False]
['text':' residual connection','line_number':760,'multiline':False]
['text':' hidden_states, present, (attentions, cross_attentions)','line_number':768,'multiline':False]
['text':' Reinitialize selected weights subject to the OpenAI GPT-2 Paper Scheme:','line_number':791,'multiline':False]
['text':'   > A modified initialization which accounts for the accumulation on the residual path with model depth. Scale','line_number':792,'multiline':False]
['text':'   > the weights of residual layers at initialization by a factor of 1/âˆšN where N is the # of residual layers.','line_number':793,'multiline':False]
['text':'   >   -- GPT-2 :: https://openai.com/blog/better-language-models/','line_number':794,'multiline':False]
['text':'','line_number':795,'multiline':False]
['text':' Reference (Megatron-LM): https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/gpt_model.py','line_number':796,'multiline':False]
['text':' Slightly different from the TF version which uses truncated_normal for initialization','line_number':802,'multiline':False]
['text':' cf https://github.com/pytorch/pytorch/pull/5617','line_number':803,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':928,'multiline':False]
['text':' create position_ids on the fly for batch generation','line_number':994,'multiline':False]
['text':' Self-attention mask.','line_number':1003,'multiline':False]
['text':' 2d mask is passed through the layers','line_number':1009,'multiline':False]
['text':' 4d mask is passed through the layers','line_number':1017,'multiline':False]
['text':' MQA models: (batch_size, query_length, n_heads, key_length)','line_number':1023,'multiline':False]
['text':' MHA models: (batch_size, n_heads, query_length, key_length)','line_number':1024,'multiline':False]
['text':' output_attentions=True can not be supported when using SDPA, and we fall back on','line_number':1028,'multiline':False]
['text':' the manual implementation that requires a 4D causal mask in all cases.','line_number':1029,'multiline':False]
['text':' gpt_bigcode using MQA has the bad taste to use a causal mask with shape','line_number':1031,'multiline':False]
['text':' [batch_size, target_length, 1, source_length], not compatible with SDPA, hence this transpose.','line_number':1032,'multiline':False]
['text':' From PyTorch 2.1 onwards, F.scaled_dot_product_attention with the memory-efficient attention backend','line_number':1036,'multiline':False]
['text':' produces nans if sequences are completely unattended in the attention mask. Details: https://github.com/pytorch/pytorch/issues/110213','line_number':1037,'multiline':False]
['text':' SDPA with a custom mask is much faster in fp16/fp32 dtype rather than bool. Cast here to floating point instead of at every layer.','line_number':1042,'multiline':False]
['text':' If a 2D or 3D attention mask is provided for the cross-attention','line_number':1054,'multiline':False]
['text':' we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]','line_number':1055,'multiline':False]
['text':' Prepare head mask if needed','line_number':1068,'multiline':False]
['text':' 1.0 in head_mask indicate we keep the head','line_number':1069,'multiline':False]
['text':' attention_probs has shape bsz x n_heads x N x N','line_number':1070,'multiline':False]
['text':' head_mask has shape n_layer x batch x n_heads x N x N','line_number':1071,'multiline':False]
['text':' Add last hidden state','line_number':1131,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1166,'multiline':False]
['text':' Omit tokens covered by past_key_values','line_number':1177,'multiline':False]
['text':' Some generation methods already pass only the last input ID','line_number':1184,'multiline':False]
['text':' Default to old behavior: keep only final ID','line_number':1188,'multiline':False]
['text':' create position_ids on the fly for batch generation','line_number':1199,'multiline':False]
['text':' if `inputs_embeds` are passed, we only want to use them in the 1st generation step','line_number':1207,'multiline':False]
['text':' Shift so that tokens < n predict n','line_number':1276,'multiline':False]
['text':' Flatten the tokens','line_number':1279,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1330,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1456,'multiline':False]
